{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtCaJV2evaljT3LoOvfJSL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Reference](https://medium.com/stanford-cs224w/self-supervised-learning-for-graphs-963e03b9f809)"
      ],
      "metadata": {
        "id": "ilZcpftUdD05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Edge Perturbation"
      ],
      "metadata": {
        "id": "fQ3DI7FrdR3k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VkUtUtwhHVxj"
      },
      "outputs": [],
      "source": [
        "class EdgePerturbation():\n",
        "    \"\"\"\n",
        "    Edge perturbation on the given graph or batched graphs. Class objects callable via \n",
        "    method :meth:`views_fn`.\n",
        "    \n",
        "    Args:\n",
        "        add (bool, optional): Set :obj:`True` if randomly add edges in a given graph.\n",
        "            (default: :obj:`True`)\n",
        "        drop (bool, optional): Set :obj:`True` if randomly drop edges in a given graph.\n",
        "            (default: :obj:`False`)\n",
        "        ratio (float, optional): Percentage of edges to add or drop. (default: :obj:`0.1`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, add=True, drop=False, ratio=0.1):\n",
        "        self.add = add\n",
        "        self.drop = drop\n",
        "        self.ratio = ratio\n",
        "        \n",
        "    def do_trans(self, data):\n",
        "        node_num, _ = data.x.size()\n",
        "        _, edge_num = data.edge_index.size()\n",
        "        perturb_num = int(edge_num * self.ratio)\n",
        "\n",
        "        edge_index = data.edge_index.detach().clone()\n",
        "        idx_remain = edge_index\n",
        "        idx_add = torch.tensor([]).reshape(2, -1).long()\n",
        "\n",
        "        if self.drop:\n",
        "            idx_remain = edge_index[:, np.random.choice(edge_num, edge_num-perturb_num, replace=False)]\n",
        "\n",
        "        if self.add:\n",
        "            idx_add = torch.randint(node_num, (2, perturb_num))\n",
        "\n",
        "        new_edge_index = torch.cat((idx_remain, idx_add), dim=1)\n",
        "        new_edge_index = torch.unique(new_edge_index, dim=1)\n",
        "\n",
        "        return Data(x=data.x, edge_index=new_edge_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Diffusion"
      ],
      "metadata": {
        "id": "U-W68i9bdUzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Diffusion():\n",
        "    \"\"\"\n",
        "    Diffusion on the given graph or batched graphs, used in \n",
        "    `MVGRL <https://arxiv.org/pdf/2006.05582v1.pdf>`_. Class objects callable via \n",
        "    method :meth:`views_fn`.\n",
        "    \n",
        "    Args:\n",
        "        mode (string, optional): Diffusion instantiation mode with two options:\n",
        "            :obj:`\"ppr\"`: Personalized PageRank; :obj:`\"heat\"`: heat kernel.\n",
        "            (default: :obj:`\"ppr\"`)\n",
        "        alpha (float, optinal): Teleport probability in a random walk. (default: :obj:`0.2`)\n",
        "        t (float, optinal): Diffusion time. (default: :obj:`5`)\n",
        "        add_self_loop (bool, optional): Set True to add self-loop to edge_index.\n",
        "            (default: :obj:`True`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mode=\"ppr\", alpha=0.2, t=5, add_self_loop=True):\n",
        "        self.mode = mode\n",
        "        self.alpha = alpha\n",
        "        self.t = t\n",
        "        self.add_self_loop = add_self_loop\n",
        "\n",
        "    def do_trans(self, data):\n",
        "        node_num, _ = data.x.size()\n",
        "        if self.add_self_loop:\n",
        "            sl = torch.tensor([[n, n] for n in range(node_num)]).t()\n",
        "            edge_index = torch.cat((data.edge_index, sl), dim=1)\n",
        "        else:\n",
        "            edge_index = data.edge_index.detach().clone()\n",
        "        \n",
        "        orig_adj = to_dense_adj(edge_index)[0]\n",
        "        orig_adj = torch.where(orig_adj>1, torch.ones_like(orig_adj), orig_adj)\n",
        "        d = torch.diag(torch.sum(orig_adj, 1))\n",
        "\n",
        "        if self.mode == \"ppr\":\n",
        "            dinv = torch.inverse(torch.sqrt(d))\n",
        "            at = torch.matmul(torch.matmul(dinv, orig_adj), dinv)\n",
        "            diff_adj = self.alpha * torch.inverse((torch.eye(orig_adj.shape[0]) - (1 - self.alpha) * at))\n",
        "\n",
        "        elif self.mode == \"heat\":\n",
        "            diff_adj = torch.exp(self.t * (torch.matmul(orig_adj, torch.inverse(d)) - 1))\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"Must choose one diffusion instantiation mode from 'ppr' and 'heat'!\")\n",
        "            \n",
        "        edge_ind, edge_attr = dense_to_sparse(diff_adj)\n",
        "\n",
        "        return Data(x=data.x, edge_index=edge_ind, edge_attr=edge_attr)"
      ],
      "metadata": {
        "id": "LhME5BM9dTtZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Node Dropping"
      ],
      "metadata": {
        "id": "T07a2OmcdX0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UniformSample():\n",
        "    \"\"\"\n",
        "    Uniformly node dropping on the given graph or batched graphs. \n",
        "    Class objects callable via method :meth:`views_fn`.\n",
        "    \n",
        "    Args:\n",
        "        ratio (float, optinal): Ratio of nodes to be dropped. (default: :obj:`0.1`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ratio=0.1):\n",
        "        self.ratio = ratio\n",
        "    \n",
        "    def do_trans(self, data):\n",
        "        \n",
        "        node_num, _ = data.x.size()\n",
        "        _, edge_num = data.edge_index.size()\n",
        "        \n",
        "        keep_num = int(node_num * (1-self.ratio))\n",
        "        idx_nondrop = torch.randperm(node_num)[:keep_num]\n",
        "        mask_nondrop = torch.zeros_like(data.x[:,0]).scatter_(0, idx_nondrop, 1.0).bool()\n",
        "        \n",
        "        edge_index, _ = subgraph(mask_nondrop, data.edge_index, relabel_nodes=True, num_nodes=node_num)\n",
        "        return Data(x=data.x[mask_nondrop], edge_index=edge_index)"
      ],
      "metadata": {
        "id": "CPP4e9XMdWdi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Random Walk based Sampling"
      ],
      "metadata": {
        "id": "-Nq-B4CKde8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RWSample():\n",
        "    \"\"\"\n",
        "    Subgraph sampling based on random walk on the given graph or batched graphs.\n",
        "    Class objects callable via method :meth:`views_fn`.\n",
        "    \n",
        "    Args:\n",
        "        ratio (float, optional): Percentage of nodes to sample from the graph.\n",
        "            (default: :obj:`0.1`)\n",
        "        add_self_loop (bool, optional): Set True to add self-loop to edge_index.\n",
        "            (default: :obj:`False`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ratio=0.1, add_self_loop=False):\n",
        "        self.ratio = ratio\n",
        "        self.add_self_loop = add_self_loop\n",
        "    \n",
        "    def do_trans(self, data):\n",
        "        node_num, _ = data.x.size()\n",
        "        sub_num = int(node_num * self.ratio)\n",
        "\n",
        "        if self.add_self_loop:\n",
        "            sl = torch.tensor([[n, n] for n in range(node_num)]).t()\n",
        "            edge_index = torch.cat((data.edge_index, sl), dim=1)\n",
        "        else:\n",
        "            edge_index = data.edge_index.detach().clone()\n",
        "\n",
        "        idx_sub = [np.random.randint(node_num, size=1)[0]]\n",
        "        idx_neigh = set([n.item() for n in edge_index[1][edge_index[0]==idx_sub[0]]])\n",
        "\n",
        "        count = 0\n",
        "        while len(idx_sub) <= sub_num:\n",
        "            count = count + 1\n",
        "            if count > node_num:\n",
        "                break\n",
        "            if len(idx_neigh) == 0:\n",
        "                break\n",
        "            sample_node = np.random.choice(list(idx_neigh))\n",
        "            if sample_node in idx_sub:\n",
        "                continue\n",
        "            idx_sub.append(sample_node)\n",
        "            idx_neigh.union(set([n.item() for n in edge_index[1][edge_index[0]==idx_sub[-1]]]))\n",
        "\n",
        "        idx_sub = torch.LongTensor(idx_sub).to(data.x.device)\n",
        "        mask_nondrop = torch.zeros_like(data.x[:,0]).scatter_(0, idx_sub, 1.0).bool()\n",
        "        edge_index, _ = subgraph(mask_nondrop, data.edge_index, relabel_nodes=True, num_nodes=node_num)\n",
        "        return Data(x=data.x[mask_nondrop], edge_index=edge_index)"
      ],
      "metadata": {
        "id": "8-c2IvrtdZTw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Node Attribute Masking"
      ],
      "metadata": {
        "id": "WNeyLgNPdiR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NodeAttrMask():\n",
        "    \"\"\"\n",
        "    Node attribute masking on the given graph or batched graphs. \n",
        "    Class objects callable via method :meth:`views_fn`.\n",
        "    \n",
        "    Args:\n",
        "        mode (string, optinal): Masking mode with three options:\n",
        "            :obj:`\"whole\"`: mask all feature dimensions of the selected node with a Gaussian distribution;\n",
        "            :obj:`\"partial\"`: mask only selected feature dimensions with a Gaussian distribution;\n",
        "            :obj:`\"onehot\"`: mask all feature dimensions of the selected node with a one-hot vector.\n",
        "            (default: :obj:`\"whole\"`)\n",
        "        mask_ratio (float, optinal): The ratio of node attributes to be masked. (default: :obj:`0.1`)\n",
        "        mask_mean (float, optional): Mean of the Gaussian distribution to generate masking values.\n",
        "            (default: :obj:`0.5`)\n",
        "        mask_std (float, optional): Standard deviation of the distribution to generate masking values. \n",
        "            Must be non-negative. (default: :obj:`0.5`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mode='whole', mask_ratio=0.1, mask_mean=0.5, mask_std=0.5, return_mask=False):\n",
        "        self.mode = mode\n",
        "        self.mask_ratio = mask_ratio\n",
        "        self.mask_mean = mask_mean\n",
        "        self.mask_std = mask_std\n",
        "        self.return_mask = return_mask\n",
        "    \n",
        "    def do_trans(self, data):\n",
        "        \n",
        "        node_num, feat_dim = data.x.size()\n",
        "        x = data.x.detach().clone()\n",
        "\n",
        "        if self.mode == \"whole\":\n",
        "            mask = torch.zeros(node_num)\n",
        "            mask_num = int(node_num * self.mask_ratio)\n",
        "            idx_mask = np.random.choice(node_num, mask_num, replace=False)\n",
        "            x[idx_mask] = torch.tensor(np.random.normal(loc=self.mask_mean, scale=self.mask_std, \n",
        "                                                        size=(mask_num, feat_dim)), dtype=torch.float32)\n",
        "            mask[idx_mask] = 1\n",
        "\n",
        "        elif self.mode == \"partial\":\n",
        "            mask = torch.zeros((node_num, feat_dim))\n",
        "            for i in range(node_num):\n",
        "                for j in range(feat_dim):\n",
        "                    if random.random() < self.mask_ratio:\n",
        "                        x[i][j] = torch.tensor(np.random.normal(loc=self.mask_mean, \n",
        "                                                                scale=self.mask_std), dtype=torch.float32)\n",
        "                        mask[i][j] = 1\n",
        "\n",
        "        elif self.mode == \"onehot\":\n",
        "            mask = torch.zeros(node_num)\n",
        "            mask_num = int(node_num * self.mask_ratio)\n",
        "            idx_mask = np.random.choice(node_num, mask_num, replace=False)\n",
        "            x[idx_mask] = torch.tensor(np.eye(feat_dim)[np.random.randint(0, feat_dim, size=(mask_num))], dtype=torch.float32)\n",
        "            mask[idx_mask] = 1\n",
        "\n",
        "        else:\n",
        "            raise Exception(\"Masking mode option '{0:s}' is not available!\".format(mode))\n",
        "\n",
        "        if self.return_mask:\n",
        "            return Data(x=x, edge_index=data.edge_index, mask=mask)\n",
        "        else:\n",
        "            return Data(x=x, edge_index=data.edge_index)"
      ],
      "metadata": {
        "id": "k8iRp7QLdhW5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Neural Networks\n"
      ],
      "metadata": {
        "id": "S_UTtHsUdzmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class GraphSAGE(nn.Module):\n",
        "    def __init__(self, feat_dim, hidden_dim, n_layers):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.acts = nn.ModuleList()\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        a = nn.ReLU()\n",
        "        for i in range(n_layers):\n",
        "            start_dim = hidden_dim if i else feat_dim\n",
        "            conv = SAGEConv(start_dim, hidden_dim)\n",
        "            self.convs.append(conv)\n",
        "            self.acts.append(a)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = self.acts[i](x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vQSH48y0dkYF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, feat_dim, hidden_dim, n_layers):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.acts = nn.ModuleList()\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        a = nn.ReLU()\n",
        "        for i in range(n_layers):\n",
        "            start_dim = hidden_dim if i else feat_dim\n",
        "            conv = GCNConv(start_dim, hidden_dim)\n",
        "            self.convs.append(conv)\n",
        "            self.acts.append(a)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = self.acts[i](x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "M1SZdTH8d1lm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, feat_dim, hidden_dim, n_layers, heads):\n",
        "        super(GAT, self).__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.acts = nn.ModuleList()\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        a = nn.LeakyReLU()\n",
        "        for i in range(n_layers):\n",
        "            start_dim = hidden_dim if i else feat_dim\n",
        "            conv = GATConv(start_dim, hidden_dim, heads=heads, concat=False)\n",
        "            self.convs.append(conv)\n",
        "            self.acts.append(a)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = self.acts[i](x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jIyJGsS5d35N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GINConv\n",
        "\n",
        "class GIN(nn.Module):\n",
        "    def __init__(self, feat_dim, hidden_dim, n_layers):\n",
        "        super(GIN, self).__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            start_dim = hidden_dim if i else feat_dim\n",
        "            mlp = nn.Sequential(\n",
        "                        nn.Linear(start_dim, hidden_dim),\n",
        "                        self.act,\n",
        "                        nn.Linear(hidden_dim, hidden_dim)\n",
        "                        )\n",
        "            conv = GINConv(mlp)\n",
        "            self.convs.append(conv)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = self.act(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "K52vn6WceEvW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch_geometric.nn import SGConv\n",
        "\n",
        "class SGC(nn.Module):\n",
        "    def __init__(self, feat_dim, hidden_dim, n_layers):\n",
        "        super(SGC, self).__init__()\n",
        "\n",
        "        self.conv = SGConv(feat_dim, hidden_dim, n_layers)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data\n",
        "\n",
        "        x = self.conv(x, edge_index)\n",
        "        x = self.act(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "sKmG-sRxeE4Z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def infonce(readout_anchor, readout_positive, tau=0.5, norm=True):\n",
        "    \"\"\"\n",
        "    The InfoNCE (NT-XENT) loss in contrastive learning. The implementation\n",
        "    follows the paper `A Simple Framework for Contrastive Learning of \n",
        "    Visual Representations <https://arxiv.org/abs/2002.05709>`.\n",
        "    Args:\n",
        "        readout_anchor, readout_positive: Tensor of shape [batch_size, feat_dim]\n",
        "        tau: Float. Usually in (0,1].\n",
        "        norm: Boolean. Whether to apply normlization.\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = readout_anchor.shape[0]\n",
        "    sim_matrix = torch.einsum(\"ik,jk->ij\", readout_anchor, readout_positive)\n",
        "\n",
        "    if norm:\n",
        "        readout_anchor_abs = readout_anchor.norm(dim=1)\n",
        "        readout_positive_abs = readout_positive.norm(dim=1)\n",
        "        sim_matrix = sim_matrix / torch.einsum(\"i,j->ij\", readout_anchor_abs, readout_positive_abs)\n",
        "\n",
        "    sim_matrix = torch.exp(sim_matrix / tau)\n",
        "    pos_sim = sim_matrix[range(batch_size), range(batch_size)]\n",
        "    loss = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n",
        "    loss = - torch.log(loss).mean()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "hRusfWSXeGrY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_expectation(masked_d_prime, positive=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        masked_d_prime: Tensor of shape [n_graphs, n_graphs] for global_global,\n",
        "                        tensor of shape [n_nodes, n_graphs] for local_global.\n",
        "        positive (bool): Set True if the d_prime is masked for positive pairs,\n",
        "                        set False for negative pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    log_2 = np.log(2.)\n",
        "    if positive:\n",
        "        score = log_2 - F.softplus(-masked_d_prime)\n",
        "    else:\n",
        "        score = F.softplus(-masked_d_prime) + masked_d_prime - log_2\n",
        "    return score\n",
        "\n",
        "def jensen_shannon(readout_anchor, readout_positive):\n",
        "    \"\"\"\n",
        "    The Jensen-Shannon Estimator of Mutual Information used in contrastive learning. The\n",
        "    implementation follows the paper `Learning deep representations by mutual information \n",
        "    estimation and maximization <https://arxiv.org/abs/1808.06670>`.\n",
        "    Note: The JSE loss implementation can produce negative values because a :obj:`-2log2` shift is \n",
        "        added to the computation of JSE, for the sake of consistency with other f-convergence losses.\n",
        "    Args:\n",
        "        readout_anchor, readout_positive: Tensor of shape [batch_size, feat_dim].\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = readout_anchor.shape[0]\n",
        "\n",
        "    pos_mask = torch.zeros((batch_size, batch_size))\n",
        "    neg_mask = torch.ones((batch_size, batch_size))\n",
        "    for graphidx in range(batch_size):\n",
        "        pos_mask[graphidx][graphidx] = 1.\n",
        "        neg_mask[graphidx][graphidx] = 0.\n",
        "\n",
        "    d_prime = torch.matmul(readout_anchor, readout_positive.t())\n",
        "\n",
        "    E_pos = get_expectation(d_prime * pos_mask, positive=True).sum()\n",
        "    E_pos = E_pos / batch_size\n",
        "    E_neg = get_expectation(d_prime * neg_mask, positive=False).sum()\n",
        "    E_neg = E_neg / (batch_size * (batch_size - 1))\n",
        "    return E_neg - E_pos"
      ],
      "metadata": {
        "id": "I07o7QwceJ_Y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GraphClassificationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Model for graph classification.\n",
        "    GNN Encoder followed by linear layer.\n",
        "    \n",
        "    Args:\n",
        "        feat_dim (int): The dimension of input node features.\n",
        "        hidden_dim (int): The dimension of node-level (local) embeddings. \n",
        "        n_layers (int, optional): The number of GNN layers in the encoder. (default: :obj:`5`)\n",
        "        gnn (string, optional): The type of GNN layer, :obj:`gcn` or :obj:`gin` or :obj:`gat`\n",
        "            or :obj:`graphsage` or :obj:`resgcn` or :obj:`sgc`. (default: :obj:`gcn`)\n",
        "        load (string, optional): The SSL model to be loaded. The GNN encoder will be\n",
        "            initialized with pretrained SSL weights, and only the classifier head will\n",
        "            be trained. Otherwise, GNN encoder and classifier head are trained end-to-end.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, feat_dim, hidden_dim, n_layers, output_dim, gnn, load=None):\n",
        "        super(GraphClassificationModel, self).__init__()\n",
        "\n",
        "        # Encoder is a wrapper class for easy instantiation of pre-implemented graph encoders.\n",
        "        self.encoder = Encoder(feat_dim, hidden_dim, n_layers=n_layers, gnn=gnn)\n",
        "\n",
        "        if load:\n",
        "            ckpt = torch.load(os.path.join(\"logs\", load, \"best_model.ckpt\"))\n",
        "            self.encoder.load_state_dict(ckpt[\"state\"])\n",
        "            for param in self.encoder.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        if gnn in [\"resgcn\", \"sgc\"]:\n",
        "            feat_dim = hidden_dim\n",
        "        else:\n",
        "            feat_dim = n_layers * hidden_dim\n",
        "        self.classifier = nn.Linear(feat_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        embeddings = self.encoder(data)\n",
        "        scores = self.classifier(embeddings)\n",
        "        return scores"
      ],
      "metadata": {
        "id": "YZ4JRnWFeZG4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import degree\n",
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "DATA_SPLIT = [0.7, 0.2, 0.1] # Train / val / test split ratio\n",
        "\n",
        "def get_max_deg(dataset):\n",
        "    \"\"\"\n",
        "    Find the max degree across all nodes in all graphs.\n",
        "    \"\"\"\n",
        "    max_deg = 0\n",
        "    for data in dataset:\n",
        "        row, col = data.edge_index\n",
        "        num_nodes = data.num_nodes\n",
        "        deg = degree(row, num_nodes)\n",
        "        deg = max(deg).item()\n",
        "        if deg > max_deg:\n",
        "            max_deg = int(deg)\n",
        "    return max_deg\n",
        "\n",
        "class CatDegOnehot(object):\n",
        "    \"\"\"\n",
        "    Adds the node degree as one hot encodings to the node features.\n",
        "    Args:\n",
        "        max_degree (int): Maximum degree.\n",
        "        in_degree (bool, optional): If set to :obj:`True`, will compute the in-\n",
        "            degree of nodes instead of the out-degree. (default: :obj:`False`)\n",
        "        cat (bool, optional): Concat node degrees to node features instead\n",
        "            of replacing them. (default: :obj:`True`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_degree, in_degree=False, cat=True):\n",
        "        self.max_degree = max_degree\n",
        "        self.in_degree = in_degree\n",
        "        self.cat = cat\n",
        "\n",
        "    def __call__(self, data):\n",
        "        idx, x = data.edge_index[1 if self.in_degree else 0], data.x\n",
        "        deg = degree(idx, data.num_nodes, dtype=torch.long)\n",
        "        deg = F.one_hot(deg, num_classes=self.max_degree + 1).to(torch.float)\n",
        "\n",
        "        if x is not None and self.cat:\n",
        "            x = x.view(-1, 1) if x.dim() == 1 else x\n",
        "            data.x = torch.cat([x, deg.to(x.dtype)], dim=-1)\n",
        "        else:\n",
        "            data.x = deg\n",
        "        return data\n",
        "\n",
        "def split_dataset(dataset, train_data_percent=1.0):\n",
        "    \"\"\"\n",
        "    Splits the data into train / val / test sets.\n",
        "    Args:\n",
        "        dataset (list): all graphs in the dataset.\n",
        "        train_data_percent (float): Fraction of training data\n",
        "            which is labelled. (default 1.0)\n",
        "    \"\"\"\n",
        "\n",
        "    random.shuffle(dataset)\n",
        "\n",
        "    n = len(dataset)\n",
        "    train_split, val_split, test_split = DATA_SPLIT\n",
        "\n",
        "    train_end = int(n * DATA_SPLIT[0] * train_data_percent)\n",
        "    val_end = train_end + int(n * DATA_SPLIT[1])\n",
        "    train_dataset, val_dataset, test_dataset = [i for i in dataset[:train_end]], [i for i in dataset[train_end:val_end]], [i for i in dataset[val_end:]]\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "# load MUTAG from TUDataset\n",
        "dataset = TUDataset(root=\"/tmp/TUDataset/MUTAG\", name=\"MUTAG\", use_node_attr=True)\n",
        "\n",
        "# expand node features by adding node degrees as one hot encodings.\n",
        "max_degree = get_max_deg(dataset)\n",
        "transform = CatDegOnehot(max_degree)\n",
        "dataset = [transform(graph) for graph in dataset]"
      ],
      "metadata": {
        "id": "pYke_RW0eMK0"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}